#!/usr/bin/env python3
"""
æ•°æ®åˆæˆè„šæœ¬ - æ”¯æŒcaptionæ³•å’Œç›´æ¥COTæ³•
Data synthesis script - supports both caption method and direct CoT method
"""

import argparse
import json
import os
import sys
import time
import random
from tqdm import tqdm
import torch
from PIL import Image
from vllm import LLM, SamplingParams
import re
import gc

def parse_args():
    parser = argparse.ArgumentParser(description='Data synthesis for puzzle-solving model')
    
    # æ•°æ®åˆæˆå‚æ•°
    parser.add_argument('--method', type=str, choices=['caption', 'direct'], default='direct',
                        help='æ•°æ®åˆæˆæ–¹æ³•: caption (ä¸¤æ­¥æ³•) æˆ– direct (ç›´æ¥æ³•)')
    parser.add_argument('--num_samples', type=int, default=1000,
                        help='ç”Ÿæˆçš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--temperature', type=float, default=0.8,
                        help='ç”Ÿæˆæ¸©åº¦')
    parser.add_argument('--filter-all-correct', action='store_true', default=False,
                        help='æ˜¯å¦è¿‡æ»¤æ‰å…¨å¯¹æ•°æ®ï¼ˆä¿ç•™æ‰€æœ‰éç©ºé¢„æµ‹ï¼‰')
    parser.add_argument('--n-candidates', type=int, default=8,
                        help='æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„å€™é€‰æ•°é‡')
    # parser.add_argument('--max-tokens', type=int, default=8192,
    #                     help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--batch-size', type=int, default=1000000,
                        help='æ¨ç†æ‰¹æ¬¡å¤§å°')
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument('--caption-model', type=str, default="/workspace/data/checkpoints/Qwen2.5-VL-72B-Instruct",
                        help='Captionæ¨¡å‹è·¯å¾„')
    parser.add_argument('--reasoning-model', type=str, default="/workspace/data/checkpoints/DeepSeek-R1-Distill-Qwen-32B",
                        help='æ¨ç†æ¨¡å‹è·¯å¾„ (captionæ³•)')
    parser.add_argument('--direct-model', type=str, default="/workspace/data/checkpoints/Qwen2.5-VL-72B-Instruct",
                        help='ç›´æ¥æ¨ç†æ¨¡å‹è·¯å¾„ (directæ³•)')
    
    # æ•°æ®è·¯å¾„
    parser.add_argument('--train-data', type=str, default="/workspace/data/datasets/train/train.jsonl",
                        help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--train-images', type=str, default="/workspace/data/datasets/train/train_images",
                        help='è®­ç»ƒå›¾åƒç›®å½•')
    
    # è¾“å‡ºè·¯å¾„
    parser.add_argument('--output-dir', type=str, default="/workspace/training_data_cache",
                        help='è®­ç»ƒæ•°æ®ç¼“å­˜ç›®å½•')
    parser.add_argument('--caption-cache-dir', type=str, default="/tmp/caption_cache",
                        help='Captionç¼“å­˜ç›®å½•')
    
    # ç¡¬ä»¶å‚æ•°
    parser.add_argument('--gpus', type=int, default=8,
                        help='ä½¿ç”¨çš„GPUæ•°é‡')
    
    return parser.parse_args()

def read_jsonl(file_path):
    """è¯»å–JSONLæ–‡ä»¶"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    return data

def write_jsonl(data, file_path):
    """å†™å…¥JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def append_jsonl(data_item, file_path):
    """è¿½åŠ å†™å…¥JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(data_item, ensure_ascii=False) + '\n')

def extract_answer_letter(pred_str):
    """ä»é¢„æµ‹æ–‡æœ¬ä¸­æå–ç­”æ¡ˆå­—æ¯"""
    pred_str = pred_str.replace("\u043a\u0438", "")
    pred = ""
    
    if "boxed" in pred_str:
        ans = pred_str.split("boxed")[-1]
        if len(ans) == 0:
            return ""
        elif ans[0] == "{":
            stack = 1
            a = ""
            for c in ans[1:]:
                if c == "{":
                    stack += 1
                    a += c
                elif c == "}":
                    stack -= 1
                    if stack == 0:
                        break
                    a += c
                else:
                    a += c
        else:
            a = ans.split("$")[0].strip()
        pred = a

    pred = re.sub(r"\n\s*", "", pred)
    if pred != "" and pred[0] == ":":
        pred = pred[1:]
    if pred != "" and pred[-1] == ".":
        pred = pred[:-1]
    if pred != "" and pred[-1] == "/":
        pred = pred[:-1]
    
    if pred and pred not in [chr(65+i) for i in range(26)]:
        pred = random.choices(["A", "B", "C", "D", "E"])[0]
    elif not pred:
        pred = random.choices(["A", "B", "C", "D", "E"])[0]
        
    return pred

def load_training_data(args):
    """åŠ è½½å¹¶é€‰æ‹©è®­ç»ƒæ•°æ®"""
    print("ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®...")
    all_data = read_jsonl(args.train_data)
    
    if len(all_data) > args.num_samples:
        selected_data = random.sample(all_data, args.num_samples)
    else:
        selected_data = all_data
        print(f"âš ï¸  è®­ç»ƒæ•°æ®åªæœ‰ {len(all_data)} æ¡ï¼Œå°‘äºè¯·æ±‚çš„ {args.num_samples} æ¡")
    
    print(f"âœ… é€‰æ‹©äº† {len(selected_data)} ä¸ªæ ·æœ¬è¿›è¡Œå¤„ç†")
    return selected_data

def generate_captions(args, samples):
    """ç”Ÿæˆå›¾åƒæè¿° (Captionæ³•çš„ç¬¬ä¸€æ­¥)"""
    print("ğŸ–¼ï¸  å¼€å§‹ç”Ÿæˆå›¾åƒæè¿°...")
    
    caption_cache_file = os.path.join(args.caption_cache_dir, "captions.jsonl")
    
    # æ£€æŸ¥ç¼“å­˜
    if os.path.exists(caption_cache_file):
        print("ğŸ“‹ å‘ç°ç¼“å­˜çš„captionï¼ŒåŠ è½½ä¸­...")
        cached_captions = read_jsonl(caption_cache_file)
        caption_dict = {item['question_id']: item['caption'] for item in cached_captions}
        
        missing_captions = [s for s in samples if s['question_id'] not in caption_dict]
        if not missing_captions:
            print("âœ… æ‰€æœ‰æ ·æœ¬çš„captionéƒ½å·²ç¼“å­˜")
            for sample in samples:
                sample['image_caption'] = caption_dict[sample['question_id']]
            return samples
        
        print(f"ğŸ”„ è¿˜éœ€è¦ä¸º{len(missing_captions)}ä¸ªæ ·æœ¬ç”Ÿæˆcaption")
        samples_to_process = missing_captions
    else:
        samples_to_process = samples
        caption_dict = {}
    
    # åˆå§‹åŒ–captionæ¨¡å‹
    print("ğŸš€ åˆå§‹åŒ–Captionæ¨¡å‹...")
    caption_llm = LLM(
        model=args.caption_model,
        tensor_parallel_size=min(args.gpus, 4),
        max_model_len=4096,
        mm_processor_kwargs={
            "min_pixels": 28 * 28,
            "max_pixels": 1280 * 28 * 28,
            "fps": 1,
        },
        gpu_memory_utilization=0.8,
        swap_space=60,
        max_num_seqs=20
    )
    
    # åˆ†æ‰¹å¤„ç†
    for i in tqdm(range(0, len(samples_to_process), args.batch_size), desc="ç”ŸæˆCaption"):
        batch = samples_to_process[i:i+args.batch_size]
        inputs = []
        
        for sample in batch:
            image_path = os.path.join(args.train_images, sample['image_file'])
            image = Image.open(image_path).convert("RGB")
            
            prompt = (
                "<|im_start|>system\nYou are a helpful assistant that provides detailed descriptions of images.<|im_end|>\n"
                "<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
                "Please describe this image in great detail, including all visually important elements, "
                "shapes, patterns, numbers, text, and spatial relationships. Focus on elements that might "
                "be relevant to solving puzzles or logical reasoning tasks."
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
            
            inputs.append({
                "prompt": prompt,
                "multi_modal_data": {"image": image}
            })
        
        sampling_params = SamplingParams(
            temperature=0.1,
            max_tokens=2048,
            n=1
        )
        
        outputs = caption_llm.generate(inputs, sampling_params=sampling_params)
        
        # ä¿å­˜caption
        for j, output in enumerate(outputs):
            question_id = batch[j]['question_id']
            caption = output.outputs[0].text
            caption_dict[question_id] = caption
            
            append_jsonl({
                'question_id': question_id,
                'caption': caption
            }, caption_cache_file)
    
    # é‡Šæ”¾æ¨¡å‹
    del caption_llm
    gc.collect()
    torch.cuda.empty_cache()
    
    # æ·»åŠ captionåˆ°æ ·æœ¬
    for sample in samples:
        sample['image_caption'] = caption_dict[sample['question_id']]
    
    print("âœ… Captionç”Ÿæˆå®Œæˆ")
    return samples

def expand_samples_by_swapping_options(samples):
    """å°†æ¯ä¸ªæ ·æœ¬æŒ‰é€‰é¡¹è¿›è¡Œæ‰©å±•ï¼šä¸æ­£ç¡®é€‰é¡¹é€ä¸€äº¤æ¢ä»¥ç”Ÿæˆå˜ä½“ã€‚

    è§„åˆ™ï¼š
    - ä¿ç•™åŸé¢˜ï¼ˆæ ‡è®°ä¸º _origï¼‰ã€‚
    - å°†æ­£ç¡®ç­”æ¡ˆä¸å…¶ä½™æ¯ä¸ªé€‰é¡¹äº¤æ¢ï¼Œç”Ÿæˆ N-1 ä¸ªæ–°é¢˜ï¼ˆæ ‡è®°ä¸º _swap_{LETTER}ï¼‰ï¼Œæœ€ç»ˆæ¯é¢˜ç”Ÿæˆ N ä¸ªæ ·æœ¬ã€‚
    - äº¤æ¢åéœ€è¦åŒæ­¥æ›´æ–° choices çš„é¡ºåºä¸ answer çš„å­—æ¯ã€‚
    - è‹¥æ ·æœ¬ä¸æ»¡è¶³è¦æ±‚ï¼ˆæ—  choicesã€answer æ— æ•ˆç­‰ï¼‰ï¼Œåˆ™åŸæ ·ä¿ç•™è¯¥æ ·æœ¬ã€‚
    """
    expanded = []
    for sample in samples:
        choices = list(sample.get('choices', []))
        answer_letter = str(sample.get('answer', '')).strip()

        if not choices or len(answer_letter) != 1 or not ('A' <= answer_letter <= 'Z'):
            expanded.append(sample)
            continue

        num_options = len(choices)
        letters = [chr(65 + i) for i in range(num_options)]
        if answer_letter not in letters:
            expanded.append(sample)
            continue

        correct_index = ord(answer_letter) - 65

        # åŸé¢˜
        original_sample = sample.copy()
        original_sample['question_id'] = f"{sample['question_id']}_orig"
        expanded.append(original_sample)

        # ä¸å…¶å®ƒé€‰é¡¹é€ä¸€äº¤æ¢
        for idx, letter in enumerate(letters):
            if idx == correct_index:
                continue

            swapped_choices = list(choices)
            swapped_choices[correct_index], swapped_choices[idx] = swapped_choices[idx], swapped_choices[correct_index]

            new_sample = sample.copy()
            new_sample['choices'] = swapped_choices
            new_sample['answer'] = letter
            new_sample['question_id'] = f"{sample['question_id']}_swap_{letter}"

            # é¿å…æ²¿ç”¨å¯èƒ½å­˜åœ¨çš„æ—§ caption
            if 'image_caption' in new_sample:
                del new_sample['image_caption']

            expanded.append(new_sample)

    return expanded

def synthesize_data_caption_method(args, samples):
    """Captionæ³•ï¼šåŸºäºæè¿°+é—®é¢˜ç”ŸæˆCoT"""
    print("ğŸ§  å¼€å§‹Captionæ³•CoTç”Ÿæˆ...")
    
    # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
    print("ğŸš€ åˆå§‹åŒ–æ¨ç†æ¨¡å‹...")
    reasoning_llm = LLM(
        model=args.reasoning_model,
        tensor_parallel_size=args.gpus,
        # max_model_len=args.max_tokens,
        max_model_len=8192,
        gpu_memory_utilization=0.8,
        swap_space=60,
        max_num_seqs=20
    )
    
    results = []
    
    for i in tqdm(range(0, len(samples), args.batch_size), desc="Captionæ³•CoTç”Ÿæˆ"):
        batch = samples[i:i+args.batch_size]
        inputs = []
        metadata = []
        
        for sample in batch:
            question = sample['question']
            choices = sample['choices']
            answer = sample['answer']
            image_caption = sample['image_caption']
            
            option_str = ""
            for idx, choice in enumerate(choices):
                letter = chr(65 + idx)
                option_str += f"{letter}. {choice}\n"
            
            prompt = (
                "<|im_start|>system\nYou are a helpful assistant that excels at logical reasoning and problem solving.<|im_end|>\n"
                "<|im_start|>user\n"
                f"Image description: {image_caption}\n\n"
                f"Question: {question}\n\n"
                f"Options:\n{option_str}\n"
                "Please think step by step and provide detailed reasoning for this problem. "
                "Use chain-of-thought reasoning to analyze the image description and question carefully. "
                "Put your final answer in \\boxed{} format with only the option letter."
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
            
            inputs.append({"prompt": prompt})
            metadata.append(sample)
        
        sampling_params = SamplingParams(
            temperature=args.temperature,
            # max_tokens=args.max_tokens,
            max_model_len=8192,
            n=args.n_candidates,
            stop=["<|im_end|>", "</s>"]
        )
        
        outputs = reasoning_llm.generate(inputs, sampling_params=sampling_params)
        
        for j, output in enumerate(outputs):
            sample = metadata[j]
            for k in range(args.n_candidates):
                generated_text = output.outputs[k].text
                predicted_answer = extract_answer_letter(generated_text)
                
                result = {
                    "question_id": f"{sample['question_id']}_{k}",
                    "question": sample['question'],
                    "choices": sample['choices'],
                    "image_file": sample['image_file'],
                    "image_caption": sample['image_caption'],
                    "response": generated_text,
                    "predicted_answer": predicted_answer,
                    "ground_truth": sample['answer']
                }
                results.append(result)
    
    del reasoning_llm
    gc.collect()
    torch.cuda.empty_cache()
    
    print("âœ… Captionæ³•CoTç”Ÿæˆå®Œæˆ")
    return results

def synthesize_data_direct_method(args, samples):
    """ç›´æ¥æ³•ï¼šåŸºäºå›¾åƒ+é—®é¢˜ç›´æ¥ç”ŸæˆCoT"""
    print("ğŸ§  å¼€å§‹ç›´æ¥æ³•CoTç”Ÿæˆ...")
    
    print("ğŸš€ åˆå§‹åŒ–å¤šæ¨¡æ€æ¨¡å‹...")
    llm = LLM(
        model=args.direct_model,
        tensor_parallel_size=args.gpus,
        # max_model_len=args.max_tokens,
        max_model_len=8192,
        mm_processor_kwargs={
            "min_pixels": 28 * 28,
            "max_pixels": 1280 * 28 * 28,
            "fps": 1,
        },
        gpu_memory_utilization=0.8,
        swap_space=60,
        max_num_seqs=20
    )
    
    results = []
    
    for i in tqdm(range(0, len(samples), args.batch_size), desc="ç›´æ¥æ³•CoTç”Ÿæˆ"):
        batch = samples[i:i+args.batch_size]
        inputs = []
        metadata = []
        
        for sample in batch:
            question = sample['question']
            choices = sample['choices']
            answer = sample['answer']
            image_path = os.path.join(args.train_images, sample['image_file'])
            image = Image.open(image_path).convert("RGB")
            
            option_str = ""
            for idx, choice in enumerate(choices):
                letter = chr(65 + idx)
                option_str += f"{letter}. {choice}\n"
            
            prompt = (
                "<|im_start|>system\nYou are a helpful assistant that excels at logical reasoning and problem solving.<|im_end|>\n"
                "<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
                f"{question}\n\n"
                f"Options:\n{option_str}\n"
                "Please analyze the image carefully and think step by step to solve this problem. "
                "Use chain-of-thought reasoning to examine all visual elements, patterns, relationships, "
                "and logical constraints. Explain your reasoning process clearly before giving your answer. "
                "Put your final answer in \\boxed{} format with only the option letter."
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
            
            inputs.append({
                "prompt": prompt,
                "multi_modal_data": {"image": image}
            })
            metadata.append(sample)
        
        sampling_params = SamplingParams(
            temperature=args.temperature,
            # max_tokens=args.max_tokens,
            max_model_len=8192,
            n=args.n_candidates,
            stop=["<|im_end|>", "</s>"]
        )
        
        outputs = llm.generate(inputs, sampling_params=sampling_params)
        
        for j, output in enumerate(outputs):
            sample = metadata[j]
            for k in range(args.n_candidates):
                generated_text = output.outputs[k].text
                predicted_answer = extract_answer_letter(generated_text)
                
                result = {
                    "question_id": f"{sample['question_id']}_{k}",
                    "question": sample['question'],
                    "choices": sample['choices'],
                    "image_file": sample['image_file'],
                    "response": generated_text,
                    "predicted_answer": predicted_answer,
                    "ground_truth": sample['answer']
                }
                results.append(result)
    
    del llm
    gc.collect()
    torch.cuda.empty_cache()
    
    print("âœ… ç›´æ¥æ³•CoTç”Ÿæˆå®Œæˆ")
    return results

def filter_and_convert_data(args, results):
    """è¿‡æ»¤æ•°æ®å¹¶è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼"""
    print("ğŸ” è¿‡æ»¤å’Œè½¬æ¢æ•°æ®...")
    
    if not args.filter_all_correct:
        # åªä¿ç•™é¢„æµ‹æ­£ç¡®çš„ç»“æœ
        filtered = [r for r in results if r['predicted_answer'] == r['ground_truth']]
        print(f"ğŸ“Š è¿‡æ»¤å‰: {len(results)} æ¡, è¿‡æ»¤å: {len(filtered)} æ¡ (åªä¿ç•™æ­£ç¡®é¢„æµ‹)")
    else:
        # ä¿ç•™æ‰€æœ‰éç©ºé¢„æµ‹çš„ç»“æœ
        filtered = [r for r in results if r['predicted_answer'] and r['predicted_answer'] != ""]
        print(f"ğŸ“Š è¿‡æ»¤å‰: {len(results)} æ¡, è¿‡æ»¤å: {len(filtered)} æ¡ (ä¿ç•™æ‰€æœ‰éç©ºé¢„æµ‹)")
    
    # è½¬æ¢ä¸ºLLaMA-Factoryæ ¼å¼
    training_data = []
    for result in filtered:
        option_str = ""
        for idx, choice in enumerate(result['choices']):
            letter = chr(65 + idx)
            option_str += f"{letter}. {choice}\n"
        
        training_sample = {
            "conversations": [
                {
                    "from": "human",
                    "value": f"<image>\n{result['question']}\n\nOptions:\n{option_str}\nPlease think step by step and provide detailed reasoning for this problem. Put your final answer in \\boxed{{}} format with only the option letter."
                },
                {
                    "from": "gpt", 
                    "value": result['response']
                }
            ],
            "images": [result['image_file']]
        }
        training_data.append(training_sample)
    
    print(f"âœ… è½¬æ¢å®Œæˆï¼Œå…± {len(training_data)} æ¡è®­ç»ƒæ ·æœ¬")
    return training_data

def main():
    args = parse_args()
    
    print("ğŸš€ å¼€å§‹æ•°æ®åˆæˆ")
    print(f"ğŸ“‹ é…ç½®å‚æ•°:")
    print(f"   - åˆæˆæ–¹æ³•: {args.method}")
    print(f"   - æ ·æœ¬æ•°é‡: {args.num_samples}")
    print(f"   - å€™é€‰æ•°é‡: {args.n_candidates}")
    print(f"   - æ¸©åº¦: {args.temperature}")
    print(f"   - GPUæ•°é‡: {args.gpus}")
    print(f"   - è¿‡æ»¤æ¨¡å¼: {'æ‰€æœ‰éç©ºé¢„æµ‹' if args.filter_all_correct else 'åªä¿ç•™æ­£ç¡®é¢„æµ‹'}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(args.caption_cache_dir, exist_ok=True)
    
    start_time = time.time()
    
    # Step 1: åŠ è½½æ•°æ®
    samples = load_training_data(args)

    # Step 2: å°†æ•°æ®æŒ‰ä¸åŒçš„é€‰é¡¹è¿›è¡Œæ‰©å±•ï¼Œå¾—åˆ°å¤šä¸ªä¸åŒçš„æ ·æœ¬
    samples = expand_samples_by_swapping_options(samples)
    
    # Step 3: æ•°æ®åˆæˆ
    if args.method == 'caption':
        # Captionæ³•ï¼šå…ˆç”Ÿæˆæè¿°ï¼Œå†ç”Ÿæˆæ¨ç†
        samples_with_captions = generate_captions(args, samples)
        results = synthesize_data_caption_method(args, samples_with_captions)
    else:
        # ç›´æ¥æ³•ï¼šç›´æ¥ç”Ÿæˆæ¨ç†
        results = synthesize_data_direct_method(args, samples)
    
    # Step 4: è¿‡æ»¤å’Œè½¬æ¢æ•°æ®
    training_data = filter_and_convert_data(args, results)
    
    # Step 5: ä¿å­˜ç»“æœ
    raw_file = os.path.join(args.output_dir, f"{args.method}_synthesis_results.jsonl")
    write_jsonl(results, raw_file)
    print(f"ğŸ“ åŸå§‹ç»“æœå·²ä¿å­˜åˆ°: {raw_file}")
    
    training_file = os.path.join(args.output_dir, f"{args.method}_training_data.jsonl")
    write_jsonl(training_data, training_file)
    print(f"ğŸ“ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_file}")
    
    end_time = time.time()
    total_time = end_time - start_time
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    
    print(f"\nğŸ‰ æ•°æ®åˆæˆå®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {hours}å°æ—¶{minutes}åˆ†é’Ÿ")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - åŸå§‹æ ·æœ¬: {len(samples)}")
    print(f"   - ç”Ÿæˆç»“æœ: {len(results)}")
    print(f"   - è®­ç»ƒæ ·æœ¬: {len(training_data)}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - åŸå§‹ç»“æœ: {raw_file}")
    print(f"   - è®­ç»ƒæ•°æ®: {training_file}")

if __name__ == "__main__":
    main()
